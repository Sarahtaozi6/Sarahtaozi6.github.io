<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Food Identification · Real-world Environments</title>
    <link rel="stylesheet" href="assets/css/styles.css" />
    <style>
        :root{
            --container: 1400px;
            --toc-w: 260px;
            --gap: 48px;
            --sticky-top: 86px;
            --content-ch: clamp(78ch, 58vw, 96ch);
            --gutter-left: 80px;
            --radius: 12px;


        }



        /* Offset in-page anchors for sticky header */
        [id]{ scroll-margin-top: calc(var(--sticky-top) + 12px); }

        main.container{
            max-width: var(--container);
            margin-inline: auto;
            padding-left: calc(20px + var(--gutter-left));
            padding-right: 20px;
        }



        .page-grid{
            display: grid;
            grid-template-columns: var(--toc-w) minmax(0,1fr);
            gap: var(--gap);
            align-items: start;
        }
        .lesson{ max-width: var(--content-ch); }

        /* Desktop TOC */
        .article-toc{
            position: sticky; top: var(--sticky-top);
            background: transparent; border: none;
            padding: 0; height: fit-content;
            font-size: 1.08rem; line-height: 1.65;
            margin-top: 0; /* will be set dynamically for alignment */
        }
        .article-toc .toc-title{ font-weight: 700; color:#0f172a; margin: 0 0 10px; font-size: 1.2rem; }
        .toc-list{ list-style: none; padding: 0; margin: 0; }
        .toc-list li{ margin: 8px 0; }
        .toc-list a{ display:block; padding:10px; min-height:40px; text-decoration:none; color:#334155; border-radius:10px; }
        .toc-list a:hover{ background:#f1f5f9; }
        .toc-list a.active{ background:#eef7ff; color:#1d4ed8; }
        .idx{ display:inline-block; width:2.2em; color:#64748b; }

        /* Typography */
        .lesson h1{
            text-align:center; width:100%;
            font-size: clamp(1.8rem, 1.3rem + 1.5vw, 2.4rem);
            margin: .4rem 0 1.8rem;
        }
        .lesson h2{ font-size: clamp(1.25rem, 1.1rem + .6vw, 1.7rem); margin: 0 0 .6rem; }
        .lesson h3{ font-size: clamp(1.02rem, .95rem + .25vw, 1.15rem); margin: 1rem 0 .4rem; }
        .lesson p{ margin: .5rem 0; }
        .lesson ul{ margin: .4rem 0 .95rem 1.25rem; }
        .lesson audio{ display:block; width:100%; margin: .8rem auto 2.8rem; }

        .lesson .img, .lesson img.img{
            max-width:100%; height:auto; display:block;
            border:none; outline:none; box-shadow:none; background:none; border-radius:12px;
        }


        .lesson .legend{ font-size:.86rem; color:#64748b; margin:.35rem 0 .6rem; }
        /* 让同一行的网格子项等高 */
        .lesson .grid.cols-2{ align-items: stretch; }   /* 覆盖 start */

        /* 让子元素占满分配到的网格高度 */
        .lesson .grid.cols-2 > *{ height: 100%; }

        /* 可选：卡片内容采用列布局，说明文字贴底更整齐 */
        .lesson .grid.cols-2 .card{ display:flex; flex-direction:column; }
        .lesson .grid.cols-2 .figcap,
        .lesson .grid.cols-2 .legend{ margin-top:auto; }




        /* Mobile TOC (collapsible) */
        .toc-mobile{ display:none; }

        @media (max-width: 1200px){
            :root{ --gutter-left: 40px; }
            main.container{ max-width: 1040px; }
        }
        @media (max-width: 1024px){
            :root{ --gutter-left: 0px; --sticky-top: 74px; }
            .page-grid{ grid-template-columns: 1fr; }
            .article-toc{ display:none; }
            .toc-mobile{
                display:block; position: sticky; top: var(--sticky-top); z-index:5;
                background:#fff; border:1px solid #e6eaf2; border-radius: var(--radius);
                padding:.25rem .5rem; margin:.25rem 0 .8rem;
                box-shadow: 0 6px 16px rgba(15,23,42,.05);
            }
            .toc-mobile summary{
                cursor:pointer; font-weight:800; color:#0f172a;
                padding:.65rem .8rem .65rem 2.2rem;
                border-radius:10px; background:#f8fafc; list-style: none;
                position: relative; user-select:none;
            }
            .toc-mobile summary::before{
                content:""; position:absolute; left:.85rem; top:50%;
                transform: translateY(-50%) rotate(0deg);
                width:0; height:0;
                border-left:7px solid currentColor;
                border-top:5px solid transparent;
                border-bottom:5px solid transparent;
                opacity:.9; transition: transform .18s ease;
            }
            .toc-mobile[open] summary{ background:#eef7ff; }
            .toc-mobile[open] summary::before{ transform: translateY(-50%) rotate(90deg); }

            .toc-mobile ol{ margin:.4rem 0 .4rem; padding:.25rem 0 .5rem; list-style:none; }
            .toc-mobile a{
                display:block; padding:.55rem .8rem; text-decoration:none; color:#334155;
                border-radius:10px; font-weight:600;
            }
            .toc-mobile a:hover{ background:#f1f5f9; }
            .toc-mobile a.active{ color:#1d4ed8; background:#eef7ff; }
            .lesson .grid.cols-2{ grid-template-columns: 1fr; }
            main.container{ padding-left:16px; padding-right:16px; }
        }
        @media (min-width:1600px){ main.container{ max-width:1600px; } }




        /* Sensor 卡片的浅蓝渐变背景（提高优先级，避免被其它规则覆盖） */
        .ai-sensor-box{
            background: linear-gradient(180deg, #f8fbff 0%, #ffffff 100%) !important;
            border: 1px solid #e6eaf2;
            border-left: 4px solid #2563eb;
            border-radius: 12px;
            padding: 1.2rem 1.4rem;
            box-shadow: 0 4px 12px rgba(37,99,235,0.08);
            margin-top: 0.8rem;
        }

        .ai-sensor-box p{ font-size:1.05rem; line-height:1.65; margin:0 0 1rem; }
        .ai-sensor-box h3{ margin-top:.8rem; font-size:1.1rem; }
        .ai-sensor-box ul{ margin-top:.5rem; padding-left:1.2rem; }
        .ai-sensor-box li{ margin-bottom:.6rem; }



        @keyframes sensorGlowShift{
            0%{background-position: 0% 50%}
            100%{background-position: 100% 50%}
        }
        /* 悬停动效（需要选择器，内联无法写 :hover） */
        #sensorCard:hover{
            transform: translateY(-4px) scale(1.01);
            box-shadow: 0 10px 28px rgba(15,23,42,.18);
        }



         #datasets .grid.cols-2{ display:grid; grid-template-columns:1fr 1fr; gap:22px; align-items:start; }
        @media (max-width:1024px){ #datasets .grid.cols-2{ grid-template-columns:1fr; } }

        #datasets .card{ background:#fff; border:1px solid #e6eaf2; border-radius:12px; box-shadow:0 8px 24px rgba(15,23,42,.06); padding:16px 18px; }

        /* 章节大标题（只作用于 #datasets 直系的 h2） */
        #datasets > h2{
            font-size: clamp(1.25rem, 1.1rem + .6vw, 1.7rem);
            margin: 0 0 .6rem;
        }

        /* 卡片里的小标题 */
        #datasets .card h2{
            margin:.1rem 0 .2rem;
            font-size:1.38rem;
        }


        #datasets h3{ margin:.1rem 0 .5rem; font-size:1.05rem; color:#0f172a; }

        /* Emoji list */
        #datasets ul{ list-style:none; padding:0; margin:.4rem 0 .2rem 0; }
        #datasets li{ display:flex; gap:10px; align-items:flex-start; margin:.5rem 0; line-height:1.7; color:#0f172a; }
        #datasets .emj{
            flex:0 0 1.6em;              /* fixed width for alignment */
            display:inline-flex; align-items:center; justify-content:center;
            font-size:1.25rem;           /* emoji size */
            line-height:1; margin-top:.2rem;
            filter: drop-shadow(0 1px 0 rgba(0,0,0,.05));
        }
        /* subtle hover “motion” */
        #datasets li:hover .emj{ transform: translateY(-1px) scale(1.05); transition: transform .15s ease; }
        #datasets .muted{ color:#64748b; }





        /* 紧一下 #datasets 与标题之间的距离（保险起见把 section 和 grid 的上边距都设 0） */
        section#datasets {
            margin-top: 0;
            padding-top: 0;
        }





    </style>
</head>
<body>
<header>
    <div class="container">
        <nav>
            <div class="brand">Food Identification · Online Tutorial</div>
            <div class="links">
                <a href="index.html">Home</a>
                <a href="refrigerator.html">Refrigerator</a>
                <a href="dl.html" class="active">Real-world Environments</a>
                <a href="quiz.html">Quiz</a>
                <a href="refs.html">References</a>
            </div>
        </nav>
    </div>
</header>

<main class="container">

    <!-- Mobile TOC -->
    <details class="toc-mobile" aria-label="Table of Contents (Mobile)">
        <summary>Table of Contents</summary>
        <ol>
            <li><a href="#bg">1. Background</a></li>
            <li><a href="#sensor">2. Sensor</a></li>

            <li><a href="#datasets">3. Datasets &amp; Models</a></li>
            <li><a href="#experimental">4. Experimental</a></li>
            <li><a href="#results-fvdnet">5. Results: YOLO vs FVDNet</a></li>
            <li><a href="#strengths">6. Strengths &amp; Applications</a></li>
            <li><a href="#limits">7. Limitations / Future Work</a></li>
        </ol>
    </details>

    <div class="page-grid">

        <!-- Left TOC (desktop) -->
        <aside class="article-toc" aria-label="Table of Contents">
            <div class="toc-title">Table of Contents</div>
            <ol class="toc-list">
                <li><a href="#bg"><span class="idx">1.</span> Background</a></li>
                <li><a href="#sensor"><span class="idx">2.</span> Sensor</a></li>

                <li><a href="#datasets"><span class="idx">3.</span> Datasets &amp; Models</a></li>
                <li><a href="#experimental"><span class="idx">4.</span> Experimental</a></li>
                <li><a href="#results-fvdnet"><span class="idx">5.</span> Results: YOLO vs FVDNet</a></li>
                <li><a href="#strengths"><span class="idx">6.</span> Strengths &amp; Applications</a></li>
                <li><a href="#limits"><span class="idx">7.</span> Limitations / Future Work</a></li>
            </ol>
        </aside>

        <!-- Right content -->
        <div>
            <section class="lesson">
                <h1>Food Identification in Real-world Environments</h1>
                <audio controls preload="metadata">
                    <source src="assets/real.mp3" type="audio/mpeg" />
                    Your browser does not support the audio element.
                </audio>

                <!-- 1. Background -->
                <section id="bg">
                    <h2>1. Background</h2>
                    <div class="grid cols-2">
                        <div class="card">
                            <p>Recognizing produce in <em>unconstrained</em> scenes—outdoor orchards, markets, kitchen countertops, even when the fridge door is opened—faces challenges such as <strong>illumination changes</strong>, <strong>occlusion</strong>, <strong>cluttered backgrounds</strong>, and wide variation in <strong>shape/size/color</strong>. Common backbones and detectors include ResNet, Inception, MobileNet, and YOLO families.</p>
                            <div class="note">
                                <strong>Core difficulties</strong>
                                <ul>
                                    <li><b>Small / distant objects</b> → reduced pixel detail.</li>
                                    <li><b>Partially covered by occlusion</b> → incomplete visual cues.</li>
                                    <li><b>Busy / cluttered backgrounds</b> → confusing context.</li>
                                </ul>
                                <p class="muted">Impact: insufficient feature-level spatial detail → hurts <em>recall</em>, <em>localization</em>, and <em>classification</em>.</p>
                            </div>
                        </div>
                        <div class="card">
                            <img class="img" src="./assets/o.png" alt="Real-world produce scenes" loading="lazy" decoding="async">
                            <div class="legend">ImageSource: AI-generated illustration.</div>
                        </div>
                    </div>
                </section>
                <h3>Examples</h3>

                <section id="examples" style="padding:32px 0; background:#fafafa;">

                    <div style="
    display:flex; justify-content:center; align-items:stretch; gap:24px;
    flex-wrap:nowrap; max-width:1120px; margin:0 auto;
  ">

                        <!-- 卡片 1 -->
                        <figure style="
      width:340px; background:#fff; border-radius:12px;
      box-shadow:0 6px 18px rgba(0,0,0,.08); padding:14px; margin:0;
      display:flex; flex-direction:column; transition:all 0.3s ease;
    "
                                onmouseover="this.style.transform='translateY(-8px)'; this.style.boxShadow='0 12px 24px rgba(0,0,0,0.12)';"
                                onmouseout="this.style.transform='translateY(0)'; this.style.boxShadow='0 6px 18px rgba(0,0,0,0.08)';">
                            <img src="assets/m.png" alt="Market scene detection example"
                                 style="width:100%; height:200px; object-fit:cover; border-radius:10px;">
                            <figcaption style="margin:12px 2px 6px; font-size:15px; line-height:1.55; color:#222; text-align:left;">
                                Detection in a busy market with illumination/occlusion/clutter.
                            </figcaption>
                            <div style="margin:0 2px; font-size:12px; color:#8a8a8a; text-align:left;">
                                Image Source: AI-generated illustration.
                            </div>
                        </figure>

                        <!-- 卡片 2 -->
                        <figure style="
      width:340px; background:#fff; border-radius:12px;
      box-shadow:0 6px 18px rgba(0,0,0,.08); padding:14px; margin:0;
      display:flex; flex-direction:column; transition:all 0.3s ease;
    "
                                onmouseover="this.style.transform='translateY(-8px)'; this.style.boxShadow='0 12px 24px rgba(0,0,0,0.12)';"
                                onmouseout="this.style.transform='translateY(0)'; this.style.boxShadow='0 6px 18px rgba(0,0,0,0.08)';">
                            <img src="assets/f3.png" alt="Open-refrigerator example"
                                 style="width:100%; height:200px; object-fit:cover; border-radius:10px;">
                            <figcaption style="margin:12px 2px 6px; font-size:15px; line-height:1.55; color:#222; text-align:left;">
                                Open-category refrigerator image-multiple small items at distance.
                            </figcaption>
                            <div style="margin:0 2px; font-size:12px; color:#8a8a8a; text-align:left;">
                                Image Source: AI-generated illustration.
                            </div>
                        </figure>

                        <!-- 卡片 3 -->
                        <figure style="
      width:340px; background:#fff; border-radius:12px;
      box-shadow:0 6px 18px rgba(0,0,0,.08); padding:14px; margin:0;
      display:flex; flex-direction:column; transition:all 0.3s ease;
    "
                                onmouseover="this.style.transform='translateY(-8px)'; this.style.boxShadow='0 12px 24px rgba(0,0,0,0.12)';"
                                onmouseout="this.style.transform='translateY(0)'; this.style.boxShadow='0 6px 18px rgba(0,0,0,0.08)';">
                            <img src="assets/p3.png" alt="Wild fruits example"
                                 style="width:100%; height:200px; object-fit:cover; border-radius:10px;">
                            <figcaption style="margin:12px 2px 6px; font-size:15px; line-height:1.55; color:#222; text-align:left;">
                                A variety of fruits captured in their natural growing environments — including orchards or farms.
                            </figcaption>
                            <div style="margin:0 2px; font-size:12px; color:#8a8a8a; text-align:left;">
                                Image Source: AI-generated illustration.
                            </div>
                        </figure>

                    </div>
                </section>


                <!-- 2. AI Sensor -->

                <section id="sensor" style="margin:0; padding:0; background:transparent !important;">
                    <h2>2. Sensor</h2>
                    <br>

                    <div id="sensorCard"
                         style="
        background: linear-gradient(135deg, #e0ecff 0%, #f7f9ff 100%) !important;
        border: 1px solid #e6eaf2 !important;
        border-left: 4px solid #3b82f6 !important;
        border-radius: 12px !important;
        padding: 16px 18px !important;
        box-shadow: 0 6px 20px rgba(15,23,42,0.12) !important;
        margin-top: .4rem !important;

        /* 动画相关（始终缓慢流动） */
        background-size: 200% 200%;
        animation: sensorGlowShift 8s ease-in-out infinite;
        /* 悬停过渡（在 :hover 里改变的属性需要 transition） */
        transition: transform .25s ease, box-shadow .25s ease;
       ">
                        <p style="font-size:1.05rem; line-height:1.65; margin:0 0 1rem;">

                            We propose an <strong>AI vision–based sensing module</strong> for recognizing produce in unconstrained scenes.
                            By combining a <strong>standard RGB camera</strong> and real-time object detection models, the system can
                            <strong>automatically recognize, track, and log fruits and vegetables</strong> across varied lighting, occlusion,
                            and background conditions—without the need for labels or manual input.
                        </p>

                        <h3 style="margin:.8rem 0 0; font-size:1.1rem;">AI Vision Sensor Overview</h3>
                        <ul style="margin:.5rem 0 0; padding-left:1.2rem;">
                            <li style="margin-bottom:.6rem;">
                                <strong>Unconstrained environments:</strong> Recognize produce in <em>real-world scenes</em> such as outdoor orchards,
                                markets, kitchen countertops, and open refrigerators, where challenges like
                                <strong>illumination change, occlusion, and background clutter</strong> occur.
                            </li>
                            <li style="margin-bottom:.6rem;">
                                <strong>RGB image format:</strong> The system operates using <strong>standard RGB images</strong> that capture natural color,
                                shape, and texture cues. This format aligns with <strong>common cameras and smartphones</strong>.
                            </li>
                            <li style="margin-bottom:.6rem;">
                                <strong>Smartphone-compatible acquisition:</strong> Datasets can be <strong>captured using mobile phone cameras</strong>,
                                offering a low-cost, reproducible way to gather training/testing samples.
                            </li>
                            <li style="margin-bottom:.6rem;">
                                <strong>Integration with deep learning:</strong> RGB frames are processed using <strong>modern vision detectors</strong>—including
                                YOLOv8, YOLO-NAS, ResNet, Inception, and MobileNet—to handle <em>small objects, occlusion, and cluttered backgrounds</em>.
                            </li>
                            <li style="margin-bottom:.6rem;">
                                <strong>Outcome:</strong> A <em>flexible RGB sensing platform</em> for static environments, improving
                                <strong>recall, localization, and classification</strong>.
                            </li>
                        </ul>
                        <div style="text-align:center; margin:1rem 0;">
                            <img src="./assets/phone.png"
                                 alt="AI Vision"
                                 style="max-width:95%; border-radius:12px; box-shadow:0 6px 18px rgba(15,23,42,0.08); transition:transform .3s ease;">
                            <div style="font-size:.85rem; color:#64748b; margin-top:.4rem;">Image Source: AI-generated illustration.</div>
                        </div>
                    </div>
                </section>

         <!-- 3. Datasets & Methods  -->

                <br>
                <br>
                <section id="datasets">
                    <h2>3. Datasets and Models</h2>
                    <div class="grid cols-2">
                        <!-- Dataset Card -->
                        <div id="fruveg67" class="card">
                            <h2>Dataset</h2>
                            <h3>FRUVEG67 (Unconstrained produce) <sup><a class="ref" href="refs.html#r4">[4]</a></sup></h3>
                            <p><strong>Dataset used in this study:</strong> <em>FRUVEG67</em> covers <b>67</b> fruit and vegetable categories with about <b>5,000 images</b>. The label space includes <b>35 vegetables</b> and <b>32 fruits</b>. Around <b>2,000</b> images are manually annotated; for the remaining images use adopt a <b>semi-supervised learning</b> pipeline (<b>SSDA</b>) that iteratively generates object annotations.</p>

                            <ul>
                                <li><span class="emj" aria-hidden="true">🗺️</span><span><strong>Scenes:</strong> markets, orchards, kitchen countertops, open refrigerators.</span></li>
                                <li><span class="emj" aria-hidden="true">🗂️</span><span><strong>Composition:</strong> 3,500 multi-object unconstrained images + 1,500 single-object images.</span></li>
                                <li><span class="emj" aria-hidden="true">🏷️</span><span><strong>Annotation:</strong> Manual (≈2k) + SSDA auto-labeling (confidence ≥ 0.5, 4 iterations).</span></li>
                                <li><span class="emj" aria-hidden="true">🧰</span><span><strong>Preprocessing:</strong> Resize 640×640 px, normalization, Gaussian smoothing, histogram equalization.</span></li>
                                <li><span class="emj" aria-hidden="true">📐</span><span><strong>Split:</strong> 70% train · 20% test · 10% validation.</span></li>
                            </ul>
                            <p class="muted" style="font-size:0.8rem; color:#64748b; margin-top:.3rem;">Note: RGB images captured by commodity/smartphone cameras are sufficient.</p>
                        </div>


                        <!-- Model Card -->
                        <div id="fvdnet" class="card">
                            <h2>Model</h2>
                            <h3>FVDNet (Fruit &amp; Vegetable Detection Network) <sup><a class="ref" href="refs.html#r4">[4]</a></sup></h3>
                            <p><strong>Model proposed in this study:</strong> <em>FVDNet</em> (Figure 1) extends <b>YOLOv7</b>, integrates a <b>semi-supervised data annotation</b> loop, and adopts a <b>Jensen–Shannon Divergence (JSD)</b>–based loss to improve small/occluded object localization.</p>

                            <ul>
                                <li><span class="emj" aria-hidden="true">🤖</span><span><strong>Architecture:</strong> YOLOv7 backbone/neck (MS-COCO pretrain); multi-scale heads (P3/P4/P5 → strides 8/16/32).</span></li>
                                <li><span class="emj" aria-hidden="true">🧪</span><span><strong>Loss:</strong> Focal Loss + JSD aligning predicted vs. GT Gaussian box distributions.</span></li>
                                <li><span class="emj" aria-hidden="true">🔁</span><span><strong>SSDA:</strong> Train → auto-label (conf ≥ 0.5) → fine-tune, repeat for 4 rounds.</span></li>
                                <li><span class="emj" aria-hidden="true">🧱</span><span><strong>Backbones tested:</strong> ResNet-152, DenseNet-169, InceptionNet.</span></li>
                                <li><span class="emj" aria-hidden="true">📊</span><span><strong>Performance:</strong> mAP <b>0.78</b> on FRUVEG67; surpasses YOLOv5/6/7 on most classes.</span></li>
                            </ul>
                        </div>
                    </div>
                </section>

                <p>Next: FRUVEG67 → FVDNet: We train and evaluate the model on this dataset.</p>
                <section id="dataset-preparation" style="font-family:'Helvetica Neue',Arial,sans-serif;margin:40px 0;">

                    <!-- Card 1 -->
                    <div
                            style="
      background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%) !important;
      border:1px solid #e6eaf2 !important;
      border-left:4px solid #3b82f6 !important;
      border-radius:12px !important;
      padding:16px 18px !important;
      box-shadow:0 10px 32px rgba(15,23,42,0.18) !important;
      margin-bottom:1.2rem !important;
      transition:transform .25s ease, box-shadow .25s ease;
    "
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 18px 48px rgba(15,23,42,0.25)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 10px 32px rgba(15,23,42,0.18)'">

                        <h2 style="font-size:1.4em;font-weight:bold;margin:0 0 6px;">1. Prepare FRUVEG67 (or similar fruit & vegetable dataset) and Preprocess</h2>
                        <p style="margin:0 0 12px;color:#374151;font-size:.96rem;">
                            Resize all images to 640×640, normalize using ImageNet statistics, and apply Gaussian smoothing, histogram equalization, and multiscale augmentation to enhance small-object detection.
                        </p>

                        <pre style="background-color:#0d1117;color:#f5f5f5;padding:16px;border-radius:10px;overflow-x:auto;font-size:0.95em;margin:0;">
# Directory structure
FRUVEG67/
  images/
    train/*.jpg
    val/*.jpg
    test/*.jpg
  labels/  # YOLO txt or COCO JSON
    train/*.txt
    val/*.txt
    test/*.txt

# Example preprocessing command
python tools/preprocess.py \
  &#45;&#45;img_root FRUVEG67/images \
  &#45;&#45;out_root FRUVEG67_proc/images \
  &#45;&#45;img_size 640 \
  &#45;&#45;normalize imagenet \
  &#45;&#45;gaussian 1 \
  &#45;&#45;histeq 1 \
  &#45;&#45;multiscale 1
    </pre>
                    </div>

                    <!-- Card 2 -->
                    <div
                            style="
      background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%) !important;
      border:1px solid #e6eaf2 !important;
      border-left:4px solid #3b82f6 !important;
      border-radius:12px !important;
      padding:16px 18px !important;
      box-shadow:0 10px 32px rgba(15,23,42,0.18) !important;
      transition:transform .25s ease, box-shadow .25s ease;
    "
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 18px 48px rgba(15,23,42,0.25)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 10px 32px rgba(15,23,42,0.18)'">

                        <h2 style="font-size:1.4em;font-weight:bold;margin:0 0 6px;">2. Start Baseline Training (Single Model: YOLOv7)</h2>
                        <p style="margin:0 0 12px;color:#374151;font-size:.96rem;">
                            Train the YOLOv7 baseline using the processed FRUVEG67 dataset for 100 epochs with batch size 16 and image size 640.
                        </p>

                        <pre style="background-color:#0d1117;color:#f5f5f5;padding:16px;border-radius:10px;overflow-x:auto;font-size:0.95em;margin:0;">
# Train YOLOv7 baseline
python train.py &#45;&#45;model yolov7 &#45;&#45;img 640 &#45;&#45;epochs 100 &#45;&#45;batch 16 \
  &#45;&#45;data configs/fruveg67.yaml &#45;&#45;device 0

# Evaluation
python eval.py &#45;&#45;weights runs/exp/best.pt \
  &#45;&#45;data configs/fruveg67.yaml &#45;&#45;iou 0.5 0.75 0.9
    </pre>
                    </div>

                </section>




                <section id="data-preprocessing" style="margin:0;padding:0;background:transparent;">
                    <div
                            style="
      background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
      border:1px solid #e6eaf2;
      border-left:4px solid #3b82f6;
      border-radius:12px;
      padding:20px 22px;
      box-shadow:0 12px 36px rgba(15,23,42,0.20);
      transition:transform .25s ease, box-shadow .25s ease;
      margin-bottom:20px;"
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 20px 56px rgba(15,23,42,0.28)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 12px 36px rgba(15,23,42,0.20)'"
                    >
                        <!-- Header -->
                        <div style="display:flex;align-items:center;gap:8px;margin-bottom:8px;">
                            <span style="display:inline-flex;width:28px;height:28px;align-items:center;justify-content:center;border-radius:999px;background:#e6efff;color:#1e40af;font-weight:700;border:1px solid #d7e4ff;">⚙️</span>
                            <h3 style="margin:0;font-size:1.08rem;">Data Preprocessing</h3>
                        </div>

                        <p style="margin:0 0 1rem;color:#475569;font-size:.95rem;">
                            Dataset split: <strong>train/val/test = 70/20/10</strong>
                        </p>

                        <!-- Content (only left side text) -->
                        <div style="max-width:800px;">
                            <h4 style="margin:0 0 .5rem;font-size:.96rem;color:#1f2a44;">Annotation Guidelines</h4>
                            <ul style="margin:0 0 1.2rem;padding-left:1.2rem;color:#374151;line-height:1.6;">
                                <li>Manually annotate at least <strong>20 images per class</strong> across varied backgrounds and lighting conditions.</li>
                                <li>Standardize image size to <strong>640×640</strong> before training.</li>
                                <li>Keep class names consistent (<em>lowercase_with_underscores</em>) to avoid duplicates.</li>
                            </ul>

                            <h4 style="margin:1rem 0 .5rem;font-size:.96rem;color:#1f2a44;">Augmentation & Normalization</h4>
                            <ul style="margin:0 0 1.2rem;padding-left:1.2rem;color:#374151;line-height:1.6;">
                                <li><strong>Denoising:</strong> Apply Gaussian smoothing to remove noise.</li>
                                <li><strong>Contrast enhancement:</strong> Use histogram equalization to highlight small or occluded objects.</li>
                                <li><strong>Multi-scale feature generation:</strong> Resize images at multiple scales (keeping aspect ratio) to improve small-object detection.</li>
                                <li><strong>Normalization:</strong> Use ImageNet mean/std
                                    (<code style="background:#eef3ff;border:1px solid #dbe7ff;border-radius:6px;padding:0 .35rem;">mean=[0.485,0.456,0.406]</code>,
                                    <code style="background:#eef3ff;border:1px solid #dbe7ff;border-radius:6px;padding:0 .35rem;">std=[0.229,0.224,0.225]</code>).
                                </li>
                            </ul>

                            <h4 style="margin:1rem 0 .5rem;font-size:.96rem;color:#1f2a44;">Processing Objective</h4>
                            <p style="margin:0;color:#475569;line-height:1.7;">
                                These preprocessing steps ensure consistency and robustness when training detection models like YOLOv7.
                                By resizing, normalizing, and augmenting images from unconstrained environments, the model better generalizes
                                to varied lighting, occlusion, and cluttered backgrounds.
                            </p>
                        </div>
                    </div>
                </section>



                <!-- ========== SSDA Section ========== -->
                <section id="ssda" style="margin:0;padding:0;background:transparent;">
                    <div
                            style="
      background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
      border:1px solid #e6eaf2;
      border-left:4px solid #3b82f6;
      border-radius:12px;
      padding:20px 22px;
      box-shadow:0 12px 36px rgba(15,23,42,0.20);
      transition:transform .25s ease, box-shadow .25s ease;
      margin-bottom:20px;"
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 20px 56px rgba(15,23,42,0.28)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 12px 36px rgba(15,23,42,0.20)'"
                    >

                        <!-- Header -->
                        <div style="display:flex;align-items:center;gap:10px;margin-bottom:6px;">
                            <span style="display:inline-flex;width:30px;height:30px;align-items:center;justify-content:center;border-radius:999px;background:#e6efff;color:#1e40af;font-weight:700;border:1px solid #d7e4ff;">🧪</span>
                            <h3 style="margin:0;font-size:1.12rem;">Semi-Supervised Data Annotation (SSDA)</h3>
                        </div>
                        <p style="margin:0 0 12px;color:#475569;font-size:.95rem;">
                            Teacher: <strong>YOLOv7</strong> (MS-COCO pretrained) · Max rounds <strong>θ = 4</strong> · Confidence threshold <strong>≥ 0.5</strong>
                        </p>

                        <!-- Tabs bar -->
                        <div style="display:flex;align-items:center;gap:0;background:#eef3fb;border:1px solid #dbe7ff;border-radius:10px;overflow:hidden;margin-bottom:12px;">
                            <button id="tab-flow"   onclick="ssdaTab('flow')"   style="flex:1;padding:10px 12px;border:0;background:#ffffff;font-weight:700;color:#0b3aa3;cursor:pointer;">Flow</button>
                            <button id="tab-pseudo" onclick="ssdaTab('pseudo')" style="flex:1;padding:10px 12px;border:0;background:transparent;font-weight:600;color:#1f2a44;cursor:pointer;">Pseudocode</button>
                            <button id="tab-cmd"    onclick="ssdaTab('cmd')"    style="flex:1;padding:10px 12px;border:0;background:transparent;font-weight:600;color:#1f2a44;cursor:pointer;">Commands</button>
                        </div>

                        <!-- Panel: Flow -->
                        <div id="panel-flow" style="display:block;">
                            <ol style="margin:0;padding:0;list-style:none;display:grid;gap:16px;">
                                <li style="display:flex;gap:12px;align-items:flex-start;">
                                    <span style="
  width:32px;height:32px;aspect-ratio:1/1;
  display:grid;place-items:center;
  border-radius:999px;background:#0f172a;color:#fff;
  font-weight:800;line-height:1;
  font-variant-numeric: tabular-nums lining-nums;
  font-feature-settings: 'tnum' 1, 'lnum' 1;
  font-family: Inter, system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
">1</span>
                                    <div style="color:#374151;line-height:1.7;">
                                        <strong>Pseudo-label the unlabeled set with the teacher.</strong><br>
                                        Run inference on all unlabeled images; if an image’s <em>maximum object confidence</em> is
                                        <strong>≥ 0.5</strong>, add the whole image (with predicted boxes) to the Train set and remove it from the pool.
                                    </div>
                                </li>
                                <li style="display:flex;gap:12px;align-items:flex-start;">
                                    <span style="width:32px;height:32px;border-radius:999px;background:#0f172a;color:#fff;display:grid;place-items:center;font-weight:800;">2</span>
                                    <div style="color:#374151;line-height:1.7;">
                                        <strong>Iterate up to four rounds.</strong><br>
                                        Fine-tune YOLOv7 on the expanded Train set, then repeat pseudo-labeling to progressively enlarge the labeled set.
                                    </div>
                                </li>
                                <li style="display:flex;gap:12px;align-items:flex-start;">
                                    <span style="
  width:32px;height:32px;aspect-ratio:1/1;
  display:grid;place-items:center;
  border-radius:999px;background:#0f172a;color:#fff;
  font-weight:800;line-height:1;
  font-variant-numeric: tabular-nums lining-nums;
  font-feature-settings: 'tnum' 1, 'lnum' 1;
  font-family: Inter, system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
">3</span>
                                    <div style="color:#374151;line-height:1.7;">
                                        <strong>Human verification to finalize.</strong><br>
                                        Manually review remaining low-confidence images (CVAT / Label Studio), fix obvious errors, and merge into the final dataset.
                                    </div>
                                </li>
                            </ol>
                        </div>

                        <!-- Panel: Pseudocode -->
                        <div id="panel-pseudo" style="display:none;">
                            <div style="
        position:relative;background:#0d1117 !important;color:#e6edf3 !important;
        border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;
        font-size:.9rem;overflow-x:auto;opacity:1 !important;mix-blend-mode:normal !important;filter:none !important;">
<pre style="margin:0;white-space:pre;background:transparent !important;color:#e6edf3 !important;">
# SSDA (θ = 4, conf ≥ 0.5)
train = labeled_set
pool  = unlabeled_set
for i in range(4):
    model.fit(train)                       # train / fine-tune YOLOv7
    keep = {img for img in pool if max_conf(img) >= 0.5}
    train |= keep                          # merge confident images
    pool  -= keep                          # shrink unlabeled pool
# Remaining images → manual review & correction
</pre>
                            </div>
                        </div>

                        <!-- Panel: Commands -->
                        <div id="panel-cmd" style="display:none;">
                            <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;margin-bottom:10px;">
<pre style="margin:0;white-space:pre;background:transparent !important;color:#e6edf3 !important;">
# 1) Train teacher (YOLOv7) on the labeled set
python train.py --model yolov7 --img 640 --epochs 100 --batch 16 \
  --data configs/fruveg67.yaml --device 0
</pre>
                            </div>

                            <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;margin-bottom:10px;">
<pre style="margin:0;white-space:pre;background:transparent !important;color:#e6edf3 !important;">
# 2) Pseudo-label unlabeled images (save boxes + confidences)
python detect.py --weights runs/exp/best.pt --source data/unlabeled \
  --save-txt --save-conf --conf 0.5 --project runs/pseudo --name round1
</pre>
                            </div>

                            <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;margin-bottom:10px;">
<pre style="margin:0;white-space:pre;background:transparent !important;color:#e6edf3 !important;">
# 3) Merge confident pseudo labels (conf ≥ 0.5) into the Train set
python tools/merge_pseudo.py \
  --pseudo runs/pseudo/round1/labels \
  --train labels/train \
  --min-conf 0.5
</pre>
                            </div>

                            <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
<pre style="margin:0;white-space:pre;background:transparent !important;color:#e6edf3 !important;">
# 4) Repeat up to 4 rounds (fine-tune, pseudo-label, merge)
for i in 1 2 3 4; do
  python train.py --model yolov7 --img 640 --epochs 50 --batch 16 \
    --data configs/fruveg67.yaml --device 0 --project runs/ssda --name round${i}
  python detect.py --weights runs/ssda/round${i}/best.pt --source data/unlabeled \
    --save-txt --save-conf --conf 0.5 --project runs/pseudo --name round${i}
  python tools/merge_pseudo.py --pseudo runs/pseudo/round${i}/labels \
    --train labels/train --min-conf 0.5
done
</pre>
                            </div>
                        </div>

                    </div>


                </section>



                <section id="model-fvdnet" style="margin:0;padding:0;background:transparent;">
                    <div
                            style="
      background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
      border:1px solid #e6eaf2;
      border-left:4px solid #3b82f6;
      border-radius:12px;
      padding:20px 22px;
      box-shadow:0 12px 36px rgba(15,23,42,0.20);
      transition:transform .25s ease, box-shadow .25s ease;
      margin-bottom:20px;"
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 20px 56px rgba(15,23,42,0.28)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 12px 36px rgba(15,23,42,0.20)'"
                    >
                        <!-- Title -->
                        <div style="display:flex;align-items:center;gap:10px;margin-bottom:8px;">
                            <span style="display:inline-flex;width:30px;height:30px;align-items:center;justify-content:center;border-radius:999px;background:#e6efff;color:#1e40af;font-weight:700;border:1px solid #d7e4ff;">🧩</span>
                            <h3 style="margin:0;font-size:1.12rem;">Model — FVDNet</h3>
                        </div>
                        <p style="margin:0 0 12px;color:#475569;font-size:.95rem;">
                            Three parallel YOLOv7 branches (grid sizes <strong>32</strong>, <strong>16</strong>, <strong>8</strong>) + JSD box loss + focal classification loss; fuse by <em>box averaging</em> and <em>class voting</em>.
                        </p>

                        <!-- Left-right layout -->
                        <div style="display:flex;flex-wrap:wrap;gap:24px;align-items:flex-start;">
                            <!-- LEFT: Architecture summary -->
                            <div style="flex:1 1 52%;min-width:280px;">
                                <h4 style="margin:0 0 .6rem;font-size:.98rem;color:#0f172a;">Architecture</h4>
                                <ul style="margin:0 0 1.1rem;padding-left:1.2rem;color:#374151;line-height:1.7;">
                                    <li><strong>Backbone/Head:</strong> YOLOv7 detector replicated into <strong>three branches</strong>.</li>
                                    <li><strong>Multi-scale design:</strong> per-branch detection grid sizes <code style="background:#eef3ff;border:1px solid #dbe7ff;border-radius:6px;padding:0 .35rem;">32</code>,
                                        <code style="background:#eef3ff;border:1px solid #dbe7ff;border-radius:6px;padding:0 .35rem;">16</code>,
                                        <code style="background:#eef3ff;border:1px solid #dbe7ff;border-radius:6px;padding:0 .35rem;">8</code> to better cover small↔large objects.</li>
                                    <li><strong>Fusion:</strong> average the three box regressions; determine final class by majority <em>vote</em>.</li>
                                </ul>

                                <h4 style="margin:0 0 .6rem;font-size:.98rem;color:#0f172a;">Losses</h4>
                                <ul style="margin:0 0 1.1rem;padding-left:1.2rem;color:#374151;line-height:1.7;">
                                    <li><strong>Bounding boxes as distributions:</strong> treat each coordinate <em>(x,y,w,h)</em> as a
                                        single-variate Gaussian with predicted <em>μ, σ</em>; compare predicted vs. GT distributions with JSD.</li>
                                    <li><strong>Overall objective:</strong> <em>L<sub>overall</sub> = L<sub>focal</sub> + L<sub>JSD</sub></em>;
                                        <em>L<sub>focal</sub></em> uses γ (0–5) and α-balancing.</li>
                                </ul>

                                <h4 style="margin:0 0 .6rem;font-size:.98rem;color:#0f172a;">Prediction Flow</h4>
                                <ol style="margin:0;padding-left:1.2rem;color:#374151;line-height:1.7;">
                                    <li>Run the three YOLOv7 branches on an image.</li>
                                    <li>Average their box outputs; apply NMS.</li>
                                    <li>Vote on class labels to get the final prediction.</li>
                                </ol>
                            </div>

                            <!-- RIGHT: Math / Pseudocode -->
                            <div style="flex:1 1 42%;min-width:260px;">
                                <!-- Equations (JSD & Overall Loss) -->
                                <h4 style="margin:0 0 .6rem;font-size:.98rem;color:#0f172a;">Equations (JSD & Overall Loss)</h4>
                                <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
  <pre style="
      margin:0;white-space:pre;
      background:#0d1117 !important;
      color:#e6edf3 !important;
      -webkit-text-fill-color:#e6edf3 !important;
      opacity:1 !important; mix-blend-mode:normal !important; filter:none !important; text-shadow:none !important;">
    <code style="background:transparent !important;color:#e6edf3 !important;-webkit-text-fill-color:#e6edf3 !important;font:inherit !important;">
# JSD between predicted P and GT Q (M = 0.5*(P+Q))
L_JSD(P,Q) = 0.5 * KL(P || M) + 0.5 * KL(Q || M)

# Overall detection objective
L_overall = L_focal + L_JSD
    </code>
  </pre>
                                </div>

                                <!-- Fusion Logic (Pseudo) -->
                                <h4 style="margin:1rem 0 .6rem;font-size:.98rem;color:#0f172a;">Fusion Logic (Pseudo)</h4>
                                <div style="background:#0d1117 !important;color:#e6edf3 !important;border-radius:10px;padding:14px;font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
  <pre style="
      margin:0;white-space:pre;
      background:#0d1117 !important;
      color:#e6edf3 !important;
      -webkit-text-fill-color:#e6edf3 !important;
      opacity:1 !important; mix-blend-mode:normal !important; filter:none !important; text-shadow:none !important;">
    <code style="background:transparent !important;color:#e6edf3 !important;-webkit-text-fill-color:#e6edf3 !important;font:inherit !important;">
# boxes_b{i}: Nx4 boxes from branch i; cls_p{i}: NxC logits
boxes  = mean([boxes_b1, boxes_b2, boxes_b3], axis=0)
scores = softmax(cls_p1) + softmax(cls_p2) + softmax(cls_p3)
labels = argmax(scores, axis=1)        # voting by sum
final  = NMS(boxes, max(scores), iou=0.5)
    </code>
  </pre>
                                </div>




                            </div>
                        </div>
                    </div>
                </section>




                <div class="card">
                    <img class="img" src="./assets/p1.png" alt="FVDNet architecture" loading="lazy" decoding="async">
                    <div class="legend">Figure 1. FVDNet training with three parallel processes (YOLOv8n variants). Source: <a href="refs.html#r4">[4]</a></div>
                </div>







                <!-- 4. Experimental -->
                <section id="experimental">
                    <h2>4. Experimental</h2>
                    <p>We evaluate FVDNet on the FRUVEG67 dataset. The goal is to assess the effectiveness of the proposed detector by swapping alternative backbones within a set of state-of-the-art models. Specifically, we vary the backbone used to extract features for FVDNet (e.g., ResNet-152 and InceptionNet). Models are trained for 100 epochs on two NVIDIA A30 GPUs. The dataset is split 70%/20%/10% for train/test/validation. For comparisons with YOLO baselines, we report performance at IoU thresholds of 0.50, 0.75, and 0.90.</p>


                    <section id="ops-quick-tabs" style="margin:0;padding:0;background:transparent;">
                        <div
                                style="background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
           border:1px solid #e6eaf2;border-left:4px solid #3b82f6;border-radius:12px;
           padding:18px 20px;box-shadow:0 12px 36px rgba(15,23,42,.20);
           transition:transform .25s,box-shadow .25s;margin-bottom:18px;"
                                onmouseover="this.style.transform='translateY(-6px)';this.style.boxShadow='0 20px 56px rgba(15,23,42,.28)'"
                                onmouseout="this.style.transform='none';this.style.boxShadow='0 12px 36px rgba(15,23,42,.20)'">

                            <h3 style="margin:0 0 10px;font-size:1.08rem;">Training · Evaluation · Extensions (Quick Guide)</h3>

                            <!-- Segmented tabs -->
                            <div style="display:flex;align-items:center;background:#eef4ff;border:1px solid #dbe7ff;
                border-radius:12px;overflow:hidden;margin-bottom:12px;">
                                <button id="ops-tab-train" onclick="opsTab('train')" aria-selected="true"
                                        style="flex:1;padding:10px;border:0;background:#ffffff;color:#0b3aa3;
                     font-weight:700;cursor:pointer;">Training</button>
                                <button id="ops-tab-eval"  onclick="opsTab('eval')"  aria-selected="false"
                                        style="flex:1;padding:10px;border:0;background:transparent;color:#25314f;
                     font-weight:600;cursor:pointer;">Evaluation</button>
                                <button id="ops-tab-ext"   onclick="opsTab('ext')"   aria-selected="false"
                                        style="flex:1;padding:10px;border:0;background:transparent;color:#25314f;
                     font-weight:600;cursor:pointer;">Extensions</button>
                            </div>

                            <!-- TRAINING panel -->
                            <div id="ops-panel-train" style="display:block;">
                                <ul style="margin:0 0 10px;padding-left:1.1rem;color:#374151;line-height:1.6;">
                                    <li><strong>Small objects:</strong> 640×640, multi-scale, sensible NMS.</li>
                                    <li><strong>Stable:</strong> EMA; keep global batch via grad-acc.</li>
                                    <li><strong>Repro:</strong> fix seeds; log SW/HW versions.</li>
                                </ul>
                                <div style="background:#0d1117!important;color:#e6edf3!important;border-radius:10px;padding:12px;
                  font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
<pre style="margin:0;white-space:pre;background:transparent!important;color:#e6edf3!important;-webkit-text-fill-color:#e6edf3!important;">
python train.py --model yolov7 --img 640 --epochs 100 --batch 16 \
  --data configs/fruveg67.yaml --device 0   # use grad-acc if VRAM is tight
</pre>
                                </div>
                            </div>

                            <!-- EVALUATION panel -->
                            <div id="ops-panel-eval" style="display:none;">
                                <ul style="margin:0 0 10px;padding-left:1.1rem;color:#374151;line-height:1.6;">
                                    <li><strong>Report:</strong> mAP, AP@0.5/0.75/0.9, per-class AP.</li>
                                </ul>
                                <div style="background:#0d1117!important;color:#e6edf3!important;border-radius:10px;padding:12px;
                  font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;margin-bottom:8px;">
<pre style="margin:0;white-space:pre;background:transparent!important;color:#e6edf3!important;-webkit-text-fill-color:#e6edf3!important;">
python eval_ensemble.py --weights runs/ens/*.pt \
  --data configs/fruveg67.yaml --iou 0.5 0.75 0.9 --save-json runs/ens/pred.json
</pre>
                                </div>
                                <div style="background:#0d1117!important;color:#e6edf3!important;border-radius:10px;padding:12px;
                  font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
<pre style="margin:0;white-space:pre;background:transparent!important;color:#e6edf3!important;-webkit-text-fill-color:#e6edf3!important;">
python tools/vis.py --pred runs/ens/pred.json --img-root FRUVEG67_proc/images
</pre>
                                </div>
                            </div>

                            <!-- EXTENSIONS panel -->
                            <div id="ops-panel-ext" style="display:none;">
                                <ul style="margin:0 10px 10px;padding-left:1.1rem;color:#374151;line-height:1.6;">
                                    <li><strong>Loss:</strong> compare JSD vs. KLD.</li>
                                    <li><strong>Backbone:</strong> ResNet-152 / DenseNet-169 / Inception.</li>
                                    <li><strong>Grid:</strong> fixed {8,16,32} vs. adaptive.</li>
                                    <li><strong>Cross-dataset:</strong> validate on VOC/COCO subsets.</li>
                                </ul>
                                <div style="background:#0d1117!important;color:#e6edf3!important;border-radius:10px;padding:12px;
                  font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
<pre style="margin:0;white-space:pre;background:transparent!important;color:#e6edf3!important;-webkit-text-fill-color:#e6edf3!important;">
python demo.py --weights runs/ens/*.pt --source fridge_samples/
</pre>
                                </div>
                            </div>

                        </div>


                    </section>


                    <img class="img" src="assets/YOLO.png" alt="YOLO results" />
                    <div class="legend">Image Source: <a href="refs.html#r4">[4]</a></div>

                    <img class="img" src="assets/FVD.png" alt="FVDNet results" />
                    <div class="legend">Image Source: <a href="refs.html#r4">[4]</a></div>
                </section>






                <!-- 5. Results -->
                <section id="results-fvdnet" class="section-mm">
                    <h2>5. Results: YOLO vs FVDNet</h2>

                    <figure class="media-card is-table">
                        <img class="media-img" src="./assets/t3.png"
                             alt="Class-wise results on FRUVEG67: YOLOv5/YOLOv6/YOLOv7 vs FVDNet">
                        <figcaption class="media-cap">
                            Class-wise scores on FRUVEG67. Columns: Y5 (YOLOv5), Y6 (YOLOv6), Y7 (YOLOv7), and
                            <strong>FVDNet</strong>. FVDNet leads on most categories, especially small/occluded items.
                            Source: <a class="ref" href="refs.html#r4">[4]</a>
                        </figcaption>
                    </figure>

                    <div class="card">
                        <h3>Key Findings</h3>
                        <ul class="mini">
                            <li><strong>Overall:</strong> FVDNet outperforms YOLOv5/6/7 on most classes, improving both localization and detection.</li>
                            <li><strong>Hard cases (small / occluded):</strong>
                                Mushroom <em>(0.19 → <strong>0.44</strong>)</em>,
                                Bitter gourd <em>(0.46 → <strong>0.63</strong>)</em>,
                                Kiwifruit <em>(0.47 → <strong>0.63</strong>)</em>,
                                Watermelon <em>(0.68 → <strong>0.78</strong>)</em>,
                                Strawberries <em>(0.64 → <strong>0.74</strong>)</em>,
                                Zucchini <em>(0.53 → <strong>0.65</strong>)</em>.
                            </li>
                            <li><strong>Common produce:</strong>
                                Potato <em>(0.74 → <strong>0.89</strong>)</em>,
                                Tomato <em>(0.81 → <strong>0.94</strong>)</em>,
                                Grapes <em>(0.59 → <strong>0.69</strong>)</em>,
                                Guava <em>(0.63 → <strong>0.70</strong>)</em>,
                                Banana <em>(0.64 → <strong>0.67</strong>)</em>,
                                Pear <em>(0.53 → <strong>0.61</strong>)</em>.
                            </li>
                            <li><strong>Low-signal classes:</strong> FVDNet still leads (e.g., Grapefruit <em>0.31 → <strong>0.42</strong></em>, Coriander <em>0.42 → <strong>0.44</strong></em>).</li>
                            <li><strong>Exceptions:</strong> A few ties/slight deficits (e.g., Ivy gourd Y6 0.48 vs FVDNet 0.47), but rare.</li>
                        </ul>
                        <div class="bq"><strong>Takeaway:</strong>
                            <li>FVDNet surpasses YOLOv7 on most classes, especially small/occluded items.</li>
                            <li>Cleaner localization and fewer misses in cluttered scenes.</li>
                            <li>FVDNet improves robustness to distance, occlusion, and clutter—often yielding double-digit AP gains.</li>
                            </div>
                    </div>
                </section>


                <!-- 6. Strengths & 7. Limits -->
                <section id="strengths" style="margin:0;padding:0;background:transparent;">
                    <h2>6. Strengths & Applications</h2>
                    <div
                            style="background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
           border:1px solid #e6eaf2;border-left:4px solid #3b82f6;border-radius:12px;
           padding:18px 20px;box-shadow:0 12px 36px rgba(15,23,42,.20);
           transition:transform .25s ease, box-shadow .25s ease; margin-bottom:18px;"
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 20px 56px rgba(15,23,42,.28)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 12px 36px rgba(15,23,42,.20)'">


                        <p style="margin:0 0 12px;color:#475569;">
                            FVDNet combines a three-branch YOLOv7 ensemble (multi-scale 32/16/8) with a JSD-based box loss and focal classification,
                            plus SSDA for low-cost labeling—yielding higher recall on small/occluded produce and cleaner localization.
                        </p>

                        <div style="display:flex;flex-wrap:wrap;gap:18px;">
                            <!-- Strengths -->
                            <div style="flex:1 1 48%;min-width:280px;">
                                <h4 style="margin:0 0 6px;font-size:.98rem;color:#0f172a;">Strengths</h4>
                                <ul style="margin:0;padding-left:1.1rem;color:#374151;line-height:1.7;">
                                    <li><strong>Small-object performance:</strong> multi-scale heads improve fruit/vegetable recall.</li>
                                    <li><strong>Robustness:</strong> handles illumination change, occlusion, and background clutter.</li>
                                    <li><strong>Stable boxes:</strong> JSD treats <em>(x,y,w,h)</em> as distributions → tighter localization.</li>
                                    <li><strong>Data efficiency:</strong> SSDA cuts manual labeling via iterative pseudo-labels.</li>
                                    <li><strong>Drop-in workflow:</strong> reuses YOLOv7 tools, training tricks, and deployment stack.</li>
                                </ul>
                            </div>

                            <!-- Applications -->
                            <div style="flex:1 1 48%;min-width:280px;">
                                <h4 style="margin:0 0 6px;font-size:.98rem;color:#0f172a;">Applications</h4>
                                <ul style="margin:0 0 8px;padding-left:1.1rem;color:#374151;line-height:1.7;">
                                    <li><strong>Smart retail:</strong> checkout-free produce recognition, inventory counts.</li>
                                    <li><strong>Home & kitchen:</strong> fridge scanning, diet logging, waste tracking.</li>
                                    <li><strong>Agrifood lines:</strong> grading/sorting on conveyors, defect pre-screen.</li>
                                    <li><strong>Markets & logistics:</strong> box-level auditing, stock analytics, cold-chain checks.</li>
                                    <li><strong>Robotics:</strong> pick-and-place perception for small items.</li>
                                    <li><strong>Education/labeling:</strong> accelerate dataset curation with SSDA rounds.</li>
                                </ul>

                                <!-- (Optional) quick demo command -->
                                <div style="background:#0d1117!important;color:#e6edf3!important;border-radius:10px;padding:12px;
                    font-family:ui-monospace,Menlo,Consolas,monospace;font-size:.9rem;overflow-x:auto;">
<pre style="margin:0;white-space:pre;background:transparent!important;color:#e6edf3!important;">
# Quick demo
python demo.py --weights runs/ens/*.pt --source samples/produce/
</pre>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

<br>

                <!-- ===================== 7. Limitations / Future Work ===================== -->
                <section id="limits" style="margin:0;padding:0;background:transparent;">
                    <h2>7. Limitations / Future Work</h2>
                    <div
                            style="background:linear-gradient(135deg,#e0ecff 0%,#f7f9ff 100%);
           border:1px solid #e6eaf2;border-left:4px solid #3b82f6;border-radius:12px;
           padding:18px 20px;box-shadow:0 12px 36px rgba(15,23,42,.20);
           transition:transform .25s ease, box-shadow .25s ease; margin-bottom:18px;"
                            onmouseover="this.style.transform='translateY(-6px)'; this.style.boxShadow='0 20px 56px rgba(15,23,42,.28)'"
                            onmouseout="this.style.transform='none'; this.style.boxShadow='0 12px 36px rgba(15,23,42,.20)'">


                        <p style="margin:0 0 10px;color:#475569;">
                            While FVDNet improves recall and localization for small/occluded produce, several trade-offs and open directions remain.
                        </p>

                        <div style="display:flex;flex-wrap:wrap;gap:18px;">
                            <!-- Limitations -->
                            <div style="flex:1 1 48%;min-width:280px;">
                                <h4 style="margin:0 0 6px;font-size:.98rem;color:#0f172a;">Limitations</h4>
                                <ul style="margin:0;padding-left:1.1rem;color:#374151;line-height:1.7;">
                                    <li><strong>RGB-only:</strong> performance drops under extreme lighting, heavy glare, or color cast.</li>
                                    <li><strong>Severe occlusion & clutter:</strong> still challenging for tiny/overlapping items.</li>
                                    <li><strong>Ensemble cost:</strong> three-branch inference increases latency and memory.</li>
                                    <li><strong>Pseudo-label noise:</strong> SSDA may propagate early mistakes (confirmation bias).</li>
                                    <li><strong>Long-tail classes:</strong> data imbalance can limit per-class AP stability.</li>
                                    <li><strong>Numerics:</strong> JSD box loss requires variance clamps / ε to remain stable.</li>
                                    <li><strong>Domain shift:</strong> retail/kitchen scenes unseen in training may degrade accuracy.</li>
                                </ul>
                            </div>

                            <!-- Future Work -->
                            <div style="flex:1 1 48%;min-width:280px;">
                                <h4 style="margin:0 0 6px;font-size:.98rem;color:#0f172a;">Future Work</h4>
                                <ul style="margin:0 0 8px;padding-left:1.1rem;color:#374151;line-height:1.7;">
                                    <li><strong>Distillation:</strong> compress the ensemble into a single fast student for edge devices.</li>
                                    <li><strong>Uncertainty-aware training:</strong> calibrated scores, IoU-aware filtering for SSDA.</li>
                                    <li><strong>Better data acquisition:</strong> active learning + hard-negative mining for long-tail classes.</li>
                                    <li><strong>Domain adaptation:</strong> style/photometric adaptation and synthetic data for new stores/kitchens.</li>
                                    <li><strong>Multi-modal sensing:</strong> add depth or NIR for illumination-robust detection.</li>
                                    <li><strong>Video & tracking:</strong> temporal smoothing and tubelet NMS to reduce flicker/misses.</li>
                                    <li><strong>Task extension:</strong> joint detection + grading/ripeness/defect segmentation.</li>
                                </ul>

                                <!-- optional note -->
                                <div style="background:#fff7ea;border:1px solid #f1e3c7;border-radius:10px;padding:10px 12px;color:#7a5200;">
                                    <strong>Deployment note:</strong> audit class bias and mislabels periodically; monitor drift and retrain with fresh data.
                                </div>
                            </div>
                        </div>
                    </div>
                </section>



        </div>
    </div>
</main>

<footer>
    <div class="container small">
        © 2025 Food Identification · Online Tutorial. All rights reserved.
    </div>
</footer>

<script>
    // Highlight active nav link based on file name
    (function() {
        let file = location.pathname.split('/').pop() || 'index.html';
        file = file.split('?')[0].split('#')[0];
        document.querySelectorAll('nav .links a').forEach(a => {
            if (a.getAttribute('href') === file) a.classList.add('active');
        });
    })();

    // -------- ONLY CHANGE: Align desktop TOC top with the "1. Background" H2 --------
    (function alignTOCInit(){
        const pageGrid = document.querySelector('.page-grid');
        const aside = document.querySelector('.article-toc');
        const firstH2 = document.querySelector('#bg h2');
        if (!pageGrid || !aside || !firstH2) return;

        function alignTOC(){
            // Compute distance between page-grid top and the first H2 top
            const gridTop = pageGrid.getBoundingClientRect().top + window.scrollY;
            const h2Top = firstH2.getBoundingClientRect().top + window.scrollY;
            const offset = Math.max(0, Math.round(h2Top - gridTop));
            aside.style.marginTop = offset + 'px';
        }

        // Initial + on events that can shift layout
        alignTOC();
        window.addEventListener('load', alignTOC, {once:false});
        window.addEventListener('resize', alignTOC);
        window.addEventListener('orientationchange', alignTOC);

        // Also observe the firstH2 and pageGrid for any size/position changes
        if (window.ResizeObserver){
            const ro = new ResizeObserver(alignTOC);
            ro.observe(firstH2);
            ro.observe(pageGrid);
        }
    })();
    // -------------------------------------------------------------------------------

    // Desktop: IO-based scroll highlight
    (function() {
        const tocLinks = document.querySelectorAll('.article-toc a');
        const secs = ['#bg','#datasets','#sensor','#examples','#experimental','#results-fvdnet','#strengths','#limits']


                .map(s => document.querySelector(s)).filter(Boolean);
        if (!tocLinks.length || !secs.length) return;
        const io = new IntersectionObserver(entries => {
            const vis = entries.filter(e => e.isIntersecting)
                .sort((a,b) => a.boundingClientRect.top - b.boundingClientRect.top)[0];
            if (!vis) return;
            const id = '#' + vis.target.id;
            tocLinks.forEach(a => a.classList.toggle('active', a.getAttribute('href') === id));
        }, { rootMargin: '-40% 0px -55% 0px', threshold: [0, 0.25, 1] });
        secs.forEach(s => io.observe(s));
    })();

    // Mobile: lock highlight on click + auto close + precise scroll offset
    (function() {
        const details = document.querySelector('.toc-mobile');
        if (!details) return;
        const links = Array.from(details.querySelectorAll('a'));
        let locked = null;

        function setActive(href){
            links.forEach(a => a.classList.toggle('active', a.getAttribute('href') === href));
        }

        function headerOffset(){
            const cssVal = getComputedStyle(document.documentElement).getPropertyValue('--sticky-top').trim();
            return (parseInt(cssVal, 10) || 0) + 10;
        }

        links.forEach(a => {
            a.addEventListener('click', e => {
                const href = a.getAttribute('href');
                const target = document.querySelector(href);
                if (!target) return;
                e.preventDefault();
                locked = href;
                setActive(href);
                details.open = false; // auto collapse after choose
                const top = target.getBoundingClientRect().top + window.scrollY - headerOffset();
                window.scrollTo({ top, behavior: 'smooth' });
            });
        });

        // Keep the active state locked; do not auto-switch on scroll
        window.addEventListener('load', () => {
            if (location.hash && details.querySelector(`a[href="${CSS.escape(location.hash)}"]`)) {
                locked = location.hash;
                setActive(locked);
            }
        });
    })();



    <!-- Tiny inline JS for tabs -->
        function ssdaTab(which){
        var tabs=['flow','pseudo','cmd'];
        tabs.forEach(function(t){
        document.getElementById('panel-'+t).style.display = (t===which)?'block':'none';
        var btn=document.getElementById('tab-'+t);
        if(btn){
        btn.style.background = (t===which)?'#ffffff':'transparent';
        btn.style.color = (t===which)?'#0b3aa3':'#1f2a44';
        btn.style.fontWeight = (t===which)?'700':'600';
    }
    });
    }



    <!-- tiny JS for segmented tabs -->

        function opsTab(which){
        const ids=['train','eval','ext'];
        ids.forEach(id=>{
        const active = id===which;
        document.getElementById('ops-panel-'+id).style.display = active?'block':'none';
        const btn = document.getElementById('ops-tab-'+id);
        btn.style.background = active ? '#ffffff' : 'transparent';
        btn.style.color      = active ? '#0b3aa3' : '#25314f';
        btn.style.fontWeight = active ? '700' : '600';
        btn.setAttribute('aria-selected', active);
    });
    }


</script>
</body>
</html>
